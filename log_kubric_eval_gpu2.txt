INFO:numexpr.utils:Note: detected 96 virtual cores but NumExpr set to maximum of 64, check "NUMEXPR_MAX_THREADS" environment variable.
INFO:numexpr.utils:Note: NumExpr detected 96 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 16.
INFO:numexpr.utils:NumExpr defaulting to 16 threads.
Seed set to: 0
Using device: cuda
Found 10 sequences.
Using 10 sequences for evaluation.
Processing sequence: 5700 with 64 pairs
  0%|          | 0/64 [00:00<?, ?it/s]  2%|▏         | 1/64 [00:01<01:34,  1.49s/it]  3%|▎         | 2/64 [00:03<01:37,  1.57s/it]  5%|▍         | 3/64 [00:04<01:35,  1.57s/it]  6%|▋         | 4/64 [00:06<01:33,  1.56s/it]  8%|▊         | 5/64 [00:07<01:31,  1.54s/it]  9%|▉         | 6/64 [00:09<01:30,  1.57s/it] 11%|█         | 7/64 [00:10<01:28,  1.56s/it] 12%|█▎        | 8/64 [00:12<01:25,  1.53s/it] 14%|█▍        | 9/64 [00:13<01:22,  1.51s/it] 16%|█▌        | 10/64 [00:15<01:21,  1.51s/it] 17%|█▋        | 11/64 [00:17<01:23,  1.57s/it] 19%|█▉        | 12/64 [00:18<01:23,  1.60s/it] 20%|██        | 13/64 [00:20<01:23,  1.63s/it] 22%|██▏       | 14/64 [00:21<01:20,  1.61s/it] 23%|██▎       | 15/64 [00:23<01:17,  1.58s/it] 25%|██▌       | 16/64 [00:24<01:11,  1.50s/it] 27%|██▋       | 17/64 [00:26<01:11,  1.52s/it] 28%|██▊       | 18/64 [00:28<01:14,  1.62s/it] 30%|██▉       | 19/64 [00:30<01:15,  1.69s/it] 31%|███▏      | 20/64 [00:31<01:13,  1.67s/it] 33%|███▎      | 21/64 [00:33<01:11,  1.67s/it] 34%|███▍      | 22/64 [00:34<01:08,  1.64s/it] 36%|███▌      | 23/64 [00:36<01:06,  1.62s/it] 38%|███▊      | 24/64 [00:38<01:06,  1.66s/it] 39%|███▉      | 25/64 [00:40<01:08,  1.76s/it] 41%|████      | 26/64 [00:42<01:08,  1.79s/it] 42%|████▏     | 27/64 [00:43<01:06,  1.80s/it] 44%|████▍     | 28/64 [00:45<01:06,  1.85s/it] 45%|████▌     | 29/64 [00:47<01:04,  1.84s/it] 47%|████▋     | 30/64 [00:49<01:01,  1.81s/it] 48%|████▊     | 31/64 [00:51<00:59,  1.81s/it] 50%|█████     | 32/64 [00:52<00:56,  1.77s/it] 52%|█████▏    | 33/64 [00:54<00:55,  1.79s/it] 53%|█████▎    | 34/64 [00:56<00:53,  1.78s/it] 55%|█████▍    | 35/64 [00:58<00:51,  1.76s/it] 56%|█████▋    | 36/64 [00:59<00:48,  1.75s/it] 58%|█████▊    | 37/64 [01:01<00:47,  1.74s/it] 59%|█████▉    | 38/64 [01:03<00:45,  1.75s/it] 61%|██████    | 39/64 [01:05<00:43,  1.73s/it] 62%|██████▎   | 40/64 [01:06<00:41,  1.73s/it] 64%|██████▍   | 41/64 [01:08<00:39,  1.73s/it] 66%|██████▌   | 42/64 [01:10<00:37,  1.69s/it] 67%|██████▋   | 43/64 [01:11<00:35,  1.71s/it] 69%|██████▉   | 44/64 [01:13<00:34,  1.73s/it] 70%|███████   | 45/64 [01:15<00:34,  1.80s/it] 72%|███████▏  | 46/64 [01:17<00:31,  1.75s/it] 73%|███████▎  | 47/64 [01:18<00:28,  1.69s/it] 75%|███████▌  | 48/64 [01:20<00:26,  1.63s/it] 77%|███████▋  | 49/64 [01:21<00:22,  1.49s/it] 78%|███████▊  | 50/64 [01:22<00:19,  1.42s/it] 80%|███████▉  | 51/64 [01:24<00:17,  1.37s/it] 81%|████████▏ | 52/64 [01:25<00:16,  1.38s/it] 83%|████████▎ | 53/64 [01:26<00:14,  1.36s/it] 84%|████████▍ | 54/64 [01:28<00:13,  1.36s/it] 86%|████████▌ | 55/64 [01:29<00:12,  1.36s/it] 88%|████████▊ | 56/64 [01:30<00:10,  1.35s/it] 89%|████████▉ | 57/64 [01:32<00:09,  1.36s/it] 91%|█████████ | 58/64 [01:33<00:08,  1.34s/it] 92%|█████████▏| 59/64 [01:34<00:06,  1.32s/it] 94%|█████████▍| 60/64 [01:36<00:05,  1.32s/it] 95%|█████████▌| 61/64 [01:37<00:04,  1.34s/it] 97%|█████████▋| 62/64 [01:38<00:02,  1.35s/it] 98%|█████████▊| 63/64 [01:40<00:01,  1.39s/it]100%|██████████| 64/64 [01:41<00:00,  1.43s/it]100%|██████████| 64/64 [01:41<00:00,  1.59s/it]
INFO:dinov2:using MLP layer as FFN
WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)

WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(

Traceback (most recent call last):
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/spatrackv2_benchmark.py", line 707, in <module>
    main()
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/spatrackv2_benchmark.py", line 483, in main
    predictions = spatrackv2_inference(collected_imgs, query_pts)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/spatrackv2_benchmark.py", line 326, in spatrackv2_inference
    ) = model.forward(video_tensor, depth=depth_tensor,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/predictor.py", line 83, in forward
    ret = self.spatrack.forward_stream(video, queries, T_org=T_,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/SpaTrack.py", line 293, in forward_stream
    out = self.forward(segment, pts_q=queries_new,
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/SpaTrack.py", line 634, in forward
    ret_track, dyn_preds, final_tracks, rgb_tracks, intrs_org, point_map_org_refined, cache = self.Track3D(imgs_raw,
                                                                                              ^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/tracker3D/TrackRefiner.py", line 802, in forward
    corr_depth_emb = self.corr_transformer[0](queried_feat.reshape(B*T*N,-1,128),
                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/tracker3D/co_tracker/utils.py", line 929, in forward
    return checkpoint.checkpoint(forward_chunk, query, target, target_rel_pos)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_compile.py", line 32, in inner
    return disable_fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py", line 745, in _fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py", line 489, in checkpoint
    return CheckpointFunction.apply(function, preserve, *args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/autograd/function.py", line 575, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/utils/checkpoint.py", line 264, in forward
    outputs = run_function(*args)
              ^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/tracker3D/co_tracker/utils.py", line 922, in forward_chunk
    new_query = self.xblock1_2(query, target).mean(dim=1, keepdim=True)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/ocean/projects/cis220039p/mdt2/jkarhade/Any4D/benchmarking/SpaTrackerV2/models/SpaTrackV2/models/tracker3D/co_tracker/utils.py", line 435, in forward
    self.norm1(x), context=self.norm_context(context), attn_bias=attn_bias
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/normalization.py", line 217, in forward
    return F.layer_norm(
           ^^^^^^^^^^^^^
  File "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py", line 2910, in layer_norm
    return torch.layer_norm(
           ^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.29 GiB. GPU 0 has a total capacity of 79.21 GiB of which 1.40 GiB is free. Process 75206 has 77.80 GiB memory in use. Of the allocated memory 72.95 GiB is allocated by PyTorch, and 3.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
